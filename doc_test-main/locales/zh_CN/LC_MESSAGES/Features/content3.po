# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, Peking University
# This file is distributed under the same license as the Hetu package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Hetu \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-04 13:35+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/Features/content3.md:1 1e338b8c0f3b4121820a74521336c468
msgid "Memory-bounded Training"
msgstr ""

#: ../../source/Features/content3.md:4 8466721b25434fff8f5cf188492628e6
msgid ""
"Since the trend of Deep Neural Networks (DNNs) is toward deeper and "
"larger, performing DNNs training on existing accelerators (e.g., GPUs) is"
" challenging due to their limited device memory capacity. Existing memory"
" management systems reduce the memory footprint via tensor offloading and"
" recomputing. However, the coarse-grained, one-tensor-at-a-time memory "
"management often incurs high peak memory usage and cannot fully utilize "
"available hardware resources."
msgstr ""

#: ../../source/Features/content3.md:6 6ee23f289a11478494e11c5a324342df
msgid ""
"We propose a fine-grained DNN memory management system that breaks apart "
"memory bottlenecks of training and greatly improves the efficiency of "
"memory-optimized DNNs training. Evaluations on differen DNN models show "
"that compared to vDNN and SuperNeurons, our system can achieve maximum "
"batch size up to 10.5× and 3.1×, and throughput up to 4.7× and 2.7× under"
" the same memory over-subscription, respectively."
msgstr ""

