# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, Peking University
# This file is distributed under the same license as the Hetu package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Hetu \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-04 13:35+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/Features/content5.md:1 8e3a54abadb9408997961779463018d3
msgid "High-performance GNN Training"
msgstr ""

#: ../../source/Features/content5.md:5 15bff0162f7a4a1a882c968bbdacfe9a
msgid ""
"Graph Neural Networks (GNNs) are powerful and flexible neural networks "
"that use the naturally sparse connectivity information of the data. "
"However, the sparsity and irregularity in graphs make it notoriously "
"difficult to perform efficient GNN computing on data parallel hardware "
"like GPU. We find that real-world graphs usually have the small-world "
"property, which means that nodes tend to create tightly knit groups "
"rather than randomly connect each other. That's why we use the locality-"
"aware algorithm METIS as the graph reorder method to improve the "
"locality. Meanwhile, it can enhance cache-efficiency and memory "
"performance to efficiently execute GCNs."
msgstr ""

#: ../../source/Features/content5.md:7 f69a3b60e98a4a74bb34b9789f0d8882
msgid ""
"In addition, for sparse matrix multiplication(SPMM), Hetu uses shared "
"memory to accelerate the calculation, which means threads in the same "
"block can share the value of sparse matrix. Also, we provide a hybrid "
"mode, which takes advantage of block sparsity of graphs. Through the "
"reorder of METIS, the graph will be partitioned into blocks. For dense "
"blocks, we use dense matrix to store and block-parse algorithm for "
"calculation. For sparse blocks, CSR format is used to store the subgraph "
"and we choose sparse matrix multiplication to calculate the product."
msgstr ""

#: ../../source/Features/content5.md:9 a6f73d84d711431c9935e8198d1f2818
msgid ""
"We compare Hetu to Deep Graph Library(DGL)  and PyTorch Geometric (PyG), "
"meanwhile, we selected 9 datasets, including 5 small datasets: PubMed, "
"Cora, Citeseer, Coauthor_phy, Blogcatalog; 4 large datasets: Reddit, "
"Proteins, Arxiv, Amazon0601. The GCN network with 64 hidden layer nodes "
"and 4 hidden layers is used for training. The results are as follows."
msgstr ""

#: ../../source/Features/content5.md:67 098a2f7934764097921851e93fff1fb0
msgid ""
"It can be seen from the table that for the small graph, the calculation "
"speed of Hetu is faster than the other two graph neural network "
"frameworks, and only using the optimized spmm can get better training "
"speed, but there is no advantage in the hybrid mode."
msgstr ""

#: ../../source/Features/content5.md:118 b03e8194535e434d8ed55ac27c4aa10c
msgid ""
"For dense large graph, the hybrid mode can speed up the training. If the "
"nodes in the graph are sparse, the optimal training speed can be obtained"
" by reorder the graph."
msgstr ""

#: ../../source/Features/content5.md:120 74b3a3a2f74b4bc4b5128c59294fec29
msgid "Here is a simple Python demo to show how to train GNN model."
msgstr ""

#: ../../source/Features/content5.md:122 ../../source/Features/content5.md:212
#: 6fb2cc487dda4048bc51dc6ca89e5dce 8b6d5dfcc34f47e5a36af3cd288666f0
msgid "Load dataset."
msgstr ""

#: ../../source/Features/content5.md:124 37e36e517a2a40529f8d8f97ff656ac0
msgid ""
"First, you need to load dataset from your file folder.If you want, you "
"can reorder the graph to accelerate computing."
msgstr ""

#: ../../source/Features/content5.md:137 ../../source/Features/content5.md:231
#: 10686806ee724bb4a6b7aefb0e43a4c6 d2531928a038424cadaac507e14b47ac
msgid "Set models."
msgstr ""

#: ../../source/Features/content5.md:139 521ec4c706b54e39adfe0a6784512cfe
msgid ""
"Then, you can get the parameters of graph and define variables  you need."
" After that, you can make your own gcn model."
msgstr ""

#: ../../source/Features/content5.md:168 ../../source/Features/content5.md:268
#: 9a179b46fc154b1291775ea340e3d362 a9055964db1d4ea6b681fd1312bafc23
msgid "Train models."
msgstr ""

#: ../../source/Features/content5.md:170 19a17112bfdb42ab8ff131d3ae6e919b
msgid "Finally, you can set the executor and train your models."
msgstr ""

#: ../../source/Features/content5.md:199 ../../source/Features/content5.md:300
#: 5a8678f0be09470c994288c4ecad816d e1e63f8b168b4b11b6cbdf65c504c0d6
msgid "The print results are as followsï¼š"
msgstr ""

#: ../../source/Features/content5.md:210 14815f44918c4bdf8e21b2e286f98504
msgid "If you want to use the hybrid mode, here is the simple Python demo."
msgstr ""

#: ../../source/Features/content5.md:214 f0821568993546c3a4cbe146fe9b38bf
msgid ""
"To use the hybird mode, you need to split the graph to get the block-"
"sparse part and coo part of the adjacency matrix. Also, you can set the "
"block size and density threshold."
msgstr ""

#: ../../source/Features/content5.md:233 aa04b83512504d15806bea3fa3e12d2b
msgid ""
"The two parts of the sparse matrix are multiplied separately. After "
"multiplication, add them to get the full result."
msgstr ""

#: ../../source/Features/content5.md:270 644294ed9be94d0dbec0176680ae584a
msgid ""
"You need to send both the sparse part of the adjacency matrix and the "
"block-sparse part into the feed dictionary."
msgstr ""

#: ../../source/Features/content5.md:311 3d161cc93d9a492da9d0119f08a143d3
msgid ""
"Our paper is under-reviewing and we will release the full details as soon"
" as possible."
msgstr ""

