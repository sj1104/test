存储受限训练
====================================

由于深度神经网络（DNN）的发展趋势是网络结构越来越深，在现有加速器（如GPU）上进行DNN训练具有挑战性，因为它们的设备存储容量有限。现有的内存管理系统通过tensor释放和重新计算来减少内存占用。然而，粗粒度、单个tensor的内存管理通常会导致高峰值内存使用率，无法充分利用可用的硬件资源。

我们提出了一个细粒度的DNN内存管理系统，它打破了训练中的内存瓶颈，极大地提高了内存优化DNN训练的效率。对不同DNN模型的评估表明，与vDNN和SuperNeurons相比，在相同的内存超额使用情况下，我们的系统可以将最大批次数量提高至10.5倍和3.1倍，吞吐量提高至4.7倍和2.7倍。



<div class="warning">
<em>我们的论文正在审查中，我们将尽快公布全部内容。</em>
</div>